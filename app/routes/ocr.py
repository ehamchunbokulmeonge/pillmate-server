from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session
import base64
import io
import json
from typing import List
from PIL import Image
from rapidfuzz import fuzz
from app.database import get_db
from app.schemas.ocr import OCRRequest, OCRResponse, MedicineMatch
from app.config import get_settings
from app.utils.aihub_loader import get_aihub_loader

router = APIRouter()
settings = get_settings()

# MVP: ê³ ì • ì‚¬ìš©ì ID (ì¸ì¦ ì—†ìŒ)
MVP_USER_ID = 1


def extract_text_from_image(image_base64: str) -> str:
    """
    ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    
    1ìˆœìœ„: Google Cloud Vision API (ì„¤ì •ëœ ê²½ìš°)
    2ìˆœìœ„: Tesseract OCR (ë¡œì»¬)
    """
    try:
        # Google Cloud Vision API ì‚¬ìš© (GOOGLE_APPLICATION_CREDENTIALS ì„¤ì • ì‹œ)
        import os
        google_creds = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')
        if google_creds:
            print(f"ğŸ”‘ Google Vision API í‚¤ íŒŒì¼: {google_creds}")
            from google.cloud import vision
            client = vision.ImageAnnotatorClient()
            image = vision.Image(content=base64.b64decode(image_base64))
            response = client.text_detection(image=image)
            if response.text_annotations:
                text = response.text_annotations[0].description
                print(f"âœ… Google Vision API í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {len(text)} ê¸€ì")
                return text
            else:
                print("âš ï¸ Google Vision API: í…ìŠ¤íŠ¸ë¥¼ ì°¾ì§€ ëª»í•¨")
        else:
            print("âš ï¸ GOOGLE_APPLICATION_CREDENTIALS í™˜ê²½ ë³€ìˆ˜ ì—†ìŒ")
    except Exception as e:
        print(f"âŒ Google Vision API ì‚¬ìš© ì‹¤íŒ¨: {e}")
    
    # Fallback: pytesseract (ê°„ë‹¨í•œ OCR)
    try:
        print("ğŸ”„ Tesseract OCRë¡œ ëŒ€ì²´...")
        import pytesseract
        from PIL import ImageEnhance, ImageFilter
        image_data = base64.b64decode(image_base64)
        image = Image.open(io.BytesIO(image_data))
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (OCR ì„±ëŠ¥ í–¥ìƒ)
        image = image.convert('L')  # í‘ë°± ë³€í™˜
        image = ImageEnhance.Contrast(image).enhance(2.0)  # ëŒ€ë¹„ ì¦ê°€
        image = image.filter(ImageFilter.SHARPEN)  # ì„ ëª…ë„ ì¦ê°€
        
        text = pytesseract.image_to_string(image, lang='kor+eng')
        print(f"âœ… Tesseract OCR ì™„ë£Œ: {len(text.strip())} ê¸€ì")
        return text.strip()
    except Exception as e:
        print(f"âŒ Tesseract OCR ì‹¤íŒ¨: {e}")
        return ""


def calculate_match_score(extracted_text: str, med_data: dict) -> float:
    """
    ì¶”ì¶œëœ í…ìŠ¤íŠ¸ì™€ ì•½ ë°ì´í„° ê°„ì˜ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° (ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜)
    """
    score = 0.0
    text_lower = extracted_text.lower()
    text_tokens = set(filter(None, [t.strip() for t in extracted_text.replace('\n', ' ').split()]))
    
    # 1. ì•½ ì´ë¦„ ë§¤ì¹­ (60ì ) - ê°€ì¤‘ì¹˜ ì¦ê°€ + í¼ì§€ ë§¤ì¹­
    drug_name = med_data.get("dl_name", "")
    drug_name_clean = drug_name.split("/")[0].strip()  # "íƒ€ì´ë°ì • 50mg/PTP" â†’ "íƒ€ì´ë°ì • 50mg"
    drug_name_base = drug_name_clean.split()[0] if drug_name_clean else ""  # "íƒ€ì´ë°ì •"
    
    # ì •í™•í•œ ë§¤ì¹­
    if drug_name_clean and drug_name_clean in extracted_text:
        score += 60
    # ê¸°ë³¸ ì•½ ì´ë¦„ë§Œ ë§¤ì¹­ (íƒ€ì´ë°ì •)
    elif drug_name_base and drug_name_base in extracted_text:
        score += 55
        # ìš©ëŸ‰ë„ ë§¤ì¹­ë˜ë©´ ë³´ë„ˆìŠ¤ (ìˆ«ìë§Œ ì¶”ì¶œí•´ì„œ ë¹„êµ)
        import re
        dose_numbers_in_text = set(re.findall(r'\d+', extracted_text))
        dose_numbers_in_name = set(re.findall(r'\d+', drug_name_clean))
        if dose_numbers_in_text & dose_numbers_in_name:  # êµì§‘í•©ì´ ìˆìœ¼ë©´
            score += 20  # ìš©ëŸ‰ ë§¤ì¹­ ë³´ë„ˆìŠ¤
    # í¼ì§€ ë§¤ì¹­ (íƒ€ì´ë°ì°œ vs íƒ€ì´ë°ì •)
    else:
        for token in text_tokens:
            if len(token) >= 2:  # 2ê¸€ì ì´ìƒë§Œ ë¹„êµ
                ratio = fuzz.ratio(token, drug_name_base)
                if ratio >= 80:  # 80% ì´ìƒ ìœ ì‚¬ë„
                    score += 50 * (ratio / 100.0)
                    break
    
    # ì˜ë¬¸ëª… ë§¤ì¹­
    drug_name_en = (med_data.get("dl_name_en") or "").lower()
    if drug_name_en:
        for token in text_tokens:
            if fuzz.ratio(token.lower(), drug_name_en) >= 85:
                score += 35
                break
    
    # 2. ê°ì¸ ì •ë³´ ë§¤ì¹­ (25ì )
    print_front = (med_data.get("print_front") or "").strip()
    print_back = (med_data.get("print_back") or "").strip()
    
    if print_front and print_front.lower() != "ë§ˆí¬" and print_front in extracted_text:
        score += 12.5
    if print_back and print_back in extracted_text:
        score += 12.5
    
    # 3. ì œì¡°ì‚¬ ë§¤ì¹­ (10ì ) - ê°€ì¤‘ì¹˜ ê°ì†Œ
    company = med_data.get("dl_company", "")
    company_en = (med_data.get("dl_company_en") or "").lower()
    
    if company and company in extracted_text:
        score += 10
    elif company_en:
        for token in text_tokens:
            if fuzz.ratio(token.lower(), company_en) >= 85:
                score += 8
                break
    
    # 4. ì„±ë¶„ëª… ë§¤ì¹­ (5ì ) - ê°€ì¤‘ì¹˜ í¬ê²Œ ê°ì†Œ
    ingredients = med_data.get("dl_material", "")
    if ingredients:
        for ingredient in ingredients.split("|"):
            ingredient = ingredient.strip()
            if ingredient and ingredient in extracted_text:
                score += 5
                break
            # í¼ì§€ ë§¤ì¹­
            for token in text_tokens:
                if len(token) >= 3 and fuzz.ratio(token, ingredient) >= 85:
                    score += 4
                    break
    
    return min(score / 100.0, 1.0)  # 0.0 ~ 1.0 ì‚¬ì´ë¡œ ì •ê·œí™”


def search_medicine_in_aihub_data(extracted_text: str) -> List[MedicineMatch]:
    """
    ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¡œ AI Hub ë°ì´í„°ì…‹ì—ì„œ ì•½ ê²€ìƒ‰
    
    ë‹¤ì¤‘ ê²€ìƒ‰ ì „ëµ:
    1. ì•½ ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰ (dl_name, dl_name_en)
    2. ê°ì¸ ì •ë³´ë¡œ ê²€ìƒ‰ (print_front, print_back)
    3. ì œì¡°ì‚¬ë¡œ ê²€ìƒ‰ (dl_company)
    4. ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° í›„ ì •ë ¬
    """
    loader = get_aihub_loader()
    
    if not loader.loaded:
        print("âš ï¸ AI Hub ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []
    
    # ë‹¤ì¤‘ ê²€ìƒ‰ ì „ëµ
    all_results = set()
    
    # 1. ì•½ ì´ë¦„ ê²€ìƒ‰ - ê°œì„ ëœ í† í° ê²€ìƒ‰
    # ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„ë¦¬ëœ í…ìŠ¤íŠ¸ë¥¼ ê°ê° ê²€ìƒ‰
    search_tokens = extracted_text.replace('\n', ' ').split()
    print(f"ğŸ” ê²€ìƒ‰ í† í°: {search_tokens}")
    
    for token in search_tokens:
        if len(token) >= 2:  # 2ê¸€ì ì´ìƒ
            name_results = loader.search_by_name(token, limit=20)
            print(f"  '{token}' ê²€ìƒ‰ ê²°ê³¼: {len(name_results)}ê°œ")
            for result in name_results:
                all_results.add(result.get("item_seq"))
    
    # 2. ê°ì¸ ì •ë³´ ì¶”ì¶œ ë° ê²€ìƒ‰ (ìˆ«ì, ì˜ë¬¸ ì¡°í•©)
    import re
    tokens = re.findall(r'[A-Za-z0-9]+', extracted_text)
    for token in tokens:
        if len(token) >= 2:  # 2ê¸€ì ì´ìƒë§Œ
            print_results = loader.search_by_print(token, limit=10)
            for result in print_results:
                all_results.add(result.get("item_seq"))
    
    print(f"âœ… ì´ {len(all_results)}ê°œ ì•½ í›„ë³´ ë°œê²¬")
    
    # 3. item_seq ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸° ë° ì ìˆ˜ ê³„ì‚°
    scored_matches = []
    for item_seq in all_results:
        med_data = loader.get_medicine_by_item_seq(item_seq)
        if med_data:
            score = calculate_match_score(extracted_text, med_data)
            if score > 0:  # ì ìˆ˜ê°€ ìˆëŠ” ê²ƒë§Œ
                drug_name = med_data.get("dl_name", "")
                print(f"  {drug_name}: {score*100:.1f}ì ")
                scored_matches.append((score, med_data))
    
    # ì ìˆ˜ìˆœ ì •ë ¬
    scored_matches.sort(reverse=True, key=lambda x: x[0])
    
    # MedicineMatch ê°ì²´ë¡œ ë³€í™˜
    matches = []
    for score, med_data in scored_matches[:10]:  # ìƒìœ„ 10ê°œë§Œ
        match = MedicineMatch(
            drug_name=med_data.get("dl_name", ""),
            drug_name_en=med_data.get("dl_name_en"),
            company=med_data.get("dl_company", ""),
            ingredients=med_data.get("dl_material", ""),
            shape=med_data.get("drug_shape"),
            color=med_data.get("color_class1"),
            print_front=med_data.get("print_front"),
            print_back=med_data.get("print_back"),
            image_url=med_data.get("img_key"),
            item_seq=str(med_data.get("item_seq", "")),
            confidence=round(score, 2)
        )
        matches.append(match)
    
    return matches


@router.post(
    "/recognize",
    response_model=OCRResponse,
    summary="ì•½ íŒ¨í‚¤ì§€ OCR ì¸ì‹",
    description="""
    ì•½ íŒ¨í‚¤ì§€ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  AI Hub ë°ì´í„°ì…‹ê³¼ ë§¤ì¹­í•©ë‹ˆë‹¤.
    
    **ê¸°ëŠ¥:**
    - Google Cloud Vision APIë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    - AI Hub ì˜ì•½í’ˆ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ ë§¤ì¹­
    - ì•½ ì´ë¦„, ì œì¡°ì‚¬, ì„±ë¶„ ì •ë³´ ì œê³µ
    - ê°ì¸ ì •ë³´ (ì•ë©´/ë’·ë©´) ë§¤ì¹­
    
    **ì‚¬ìš© ë°©ë²•:**
    1. ì•½ íŒ¨í‚¤ì§€ ì‚¬ì§„ ì´¬ì˜
    2. Base64ë¡œ ì¸ì½”ë”©í•˜ì—¬ ì „ì†¡
    3. ë§¤ì¹­ëœ ì•½ ì •ë³´ í™•ì¸
    """
)
async def recognize_medicine_package(
    ocr_data: OCRRequest,
    db: Session = Depends(get_db)
):
    """ì•½ íŒ¨í‚¤ì§€ ì´ë¯¸ì§€ ì¸ì‹ ë° ë§¤ì¹­"""
    try:
        # 1. Google Cloud Vision APIë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        extracted_text = extract_text_from_image(ocr_data.image_base64)
        
        # 2. AI Hub ë°ì´í„°ì…‹ì—ì„œ ì•½ ê²€ìƒ‰
        matched_medicines = search_medicine_in_aihub_data(extracted_text)
        
        return OCRResponse(
            extracted_text=extracted_text,
            detected_medicines=matched_medicines,
            success=True
        )
        
    except Exception as e:
        return OCRResponse(
            extracted_text="",
            detected_medicines=[],
            success=False,
            error_message=f"OCR ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
        )


@router.post(
    "/search",
    summary="ì•½ ì´ë¦„ìœ¼ë¡œ ê²€ìƒ‰",
    description="""
    ì•½ ì´ë¦„, ì œì¡°ì‚¬, ê°ì¸ ì •ë³´ë¡œ AI Hub ë°ì´í„°ì…‹ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    
    **ê²€ìƒ‰ ê°€ëŠ¥í•œ ì •ë³´:**
    - ì•½ ì´ë¦„ (í•œê¸€/ì˜ë¬¸)
    - ì œì¡°ì‚¬ëª…
    - ì•ë©´/ë’·ë©´ ê°ì¸ ë¬¸ì
    """
)
async def search_medicine_by_name(
    query: str,
    db: Session = Depends(get_db)
):
    """ì•½ ì´ë¦„ìœ¼ë¡œ AI Hub ë°ì´í„°ì…‹ ê²€ìƒ‰"""
    try:
        # AI Hub ë°ì´í„°ì…‹ì—ì„œ ê²€ìƒ‰
        results = search_medicine_in_aihub_data(query)
        
        return {
            "query": query,
            "count": len(results),
            "results": [result.model_dump() for result in results]
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}"
        )
